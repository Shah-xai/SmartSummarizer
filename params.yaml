TrainingArguments:
  num_train_epochs: 2
  warmup_steps: 500
  warmup_ratio: 0.04
  per_device_train_batch_size: 1
  gradient_accumulation_steps: 8
  learning_rate: 0.0001
  lr_scheduler_type: cosine
  weight_decay: 0.01
  logging_steps: 10
  eval_strategy: steps
  metric_for_best_model: eval_loss
  load_best_model_at_end: True
  eval_steps: 500
  save_steps: 1000
  fp16: True
  greater_is_better: False